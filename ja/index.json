[{"uri":"https://gorgonia.org/ja/about/computation-graph/","title":"計算グラフ","tags":[],"description":"Graphs and *Nodes","content":" Gorgonia はグラフベース _Note_：この記事は このブログ投稿 からインスピレーションを得ています。\nTensorflow や Theano など殆どの深層学習ライブラリと同様に、Gorgonia は方程式がグラフで表現できるという概念に依存しています。\n方程式グラフをプログラマーが操作できる ExprGraph オブジェクトとして公開します。\nですので以下の様に書く代わりに:\nfunc main() { fmt.Printf(\u0026#34;%v\u0026#34;, 1+1) } プログラマはこう書くのです:\nfunc main() { // Create a graph. \tg := gorgonia.NewGraph() // Create a node called \u0026#34;x\u0026#34; with the value 1. \tx := gorgonia.NodeFromAny(g, 1, gorgonia.WithName(\u0026#34;x\u0026#34;)) // Create a node called \u0026#34;y\u0026#34; with the value 1. \ty := gorgonia.NodeFromAny(g, 1, gorgonia.WithName(\u0026#34;y\u0026#34;)) // z := x + y \tz := gorgonia.Must(gorgonia.Add(x, y)) // Create a VM to execute the graph. \tvm := gorgonia.NewTapeMachine(g) // Run the VM. Errors are not checked. \tvm.RunAll() // Print the value of z. \tfmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) } 数値の安定性 方程式 $y = log(1+x)$ を考えてみてください。 この方程式は数値的には安定していません- $x$ の値が非常に小さい場合、答えはおそらく間違っています。 これは float64 の設計方法が原因です。float64 には、1 と 1 + 10e-16 を区別するのに十分なビットがありません。 実際には、Go でそれを正しく行うには標準ライブラリ関数 math.Log1p を使用します。 次の簡単なプログラムで表示できます:\nfunc main() { fmt.Printf(\u0026#34;%v\\n\u0026#34;, math.Log(1.0+10e-16)) fmt.Printf(\u0026#34;%v\\n\u0026#34;, math.Log1p(10e-16)) }1.110223024625156e-15 // wrong 9.999999999999995e-16 // correct Gorgonia は数値の安定性を確保するために最適な実装を使用する事でこれを処理します。\nExpGraph と *Node ExprGraph は方程式を保持するオブジェクトです。このグラフの頂点は方程式を構成する具体化すべき値または演算子です。 これらの頂点は \u0026ldquo;ノード\u0026rdquo; と呼ばれる構造によって表されます。グラフはこの構造へのポインタを保持しています。\n方程式を作成するには ExprGraph を作成しいくつかの *Node を追加し、それらを互いに結びつける必要があります。\n幸いなことにノード間の接続を手動で管理する必要はありません。\nプレースホルダとオペレータ Node はいくつかの値を保持できます (Value はスカラーやテンソルなどの具象型を表す Go インタフェースです)。 ただし オペレータ も保持できます。\n計算時には値はグラフに沿って流れ、オペレータを含む各ノードは対応するコードを実行し、値を対応するノードに設定します。\n勾配計算 さらに Gorgonia は数式微分と自動微分の両方を行うことができます。 この ページ ではその仕組みについて詳しく説明しています。\n"},{"uri":"https://gorgonia.org/ja/reference/exprgraph/","title":"グラフと Exprgraph","tags":[],"description":"","content":"計算グラフまたは式グラフについて多くのことが言われています。しかしそれらは正しいのでしょうか？あなたが望む数学表現の AST と考えてください。上記の例のグラフを次に示します(ただし代わりにベクトルとスカラーを追加します):\nちなみに Gorgonia には素敵なグラフ印刷機能が備わっています。方程式 $y = x^2$ とその派生のグラフの例を次に示します:\nグラフを読むのは簡単です。式は下から上に構築され、派生は上から下に構築されます。これにより各ノードの導関数はほぼ同じレベルとなります。\n赤枠のノードはそれがルート node であることを示します。緑のアウトラインノードは葉 node であることを示します。背景が黄色のノードは入力ノードであることを示しています。点線の矢印はどのノードがポイント先ノードのグラデーションノードであるかを示しています。\n具体的には c42011e840 ($\\frac{\\partial{y}}{\\partial{x}}$) が入力 c42011e000 (つまり $x$) の勾配ノードであると言います。\n"},{"uri":"https://gorgonia.org/ja/tutorials/hello-world/","title":"こんにちわ世界","tags":[],"description":"","content":" これはGorgoniaでとても簡単な計算を行うための段階的なチュートリアルです。\n私たちのゴールはGorgoniaのすべての配管を使用して簡単な操作を行うことです:\n$ f(x,y) = x + y $\n値は x = 2 と y = 5\nどの様に動作するか x + y = z の評価はグラフで表す事ができます:\ngraph LR; z[z] -- add(Round edge) add[+] -- x add[+] -- y  結果を計算する為に4つのステップを使います:\n Gorgonia で式の様なグラフを作成する nodes x と y に幾つかの値を設定する gorgonia vm上でグラフを起動する node zからvalueを取り出す *  グラフの作成 以下の方法で空の式グラフを作成します:\ng := gorgonia.NewGraph() ノードの作成 いくつかのノードを作成しそれらを ExprGraph に関連付けます。\nvar x, y, z *gorgonia.Node プレースホルダの作成 xとyはスカラー変数です。対応するノードは次のように作成できます:\nx = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;y\u0026#34;)) 関数は引数としてexprgraphを取ります; 結果のノードは自動的にグラフに関連付けられます。\n 次に加算演算子を作成します。この演算子は2つのノードを取り新しいノードzを返します:\nif z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } 戻り値のノードzはgがzまたはAdd関数に渡されていない場合でもグラフには追加されます。\n 値の設定 式 z = x + y を表す ExprGraph ができました。ではxとyにいくつかの値を割り当てます。\n関数 Let を使います:\ngorgonia.Let(x, 2.0) gorgonia.Let(y, 2.5) グラフの実行 グラフを実行して結果を計算するには VM をインスタンス化する必要があります。 TapeMachineを使いましょう:\nmachine := gorgonia.NewTapeMachine(g) defer machine.Close() そしてグラフの実行:\nif err = machine.RunAll(); err != nil { log.Fatal(err) } 2回目の実行が必要な場合vmオブジェクトのReset()メソッドを呼び出すことが必須です: machine.Reset()\n 値の取得 これでノードzが結果を保持します。 Valueを抽出するにはValue()メソッドを呼び出します:\nfmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) この場合float64を保持するinterface{}を返すz.Value().Data()を呼び出して、基になる\u0026rdquo;Go\u0026rdquo;値にアクセスすることもできます。\n 最終結果 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;gorgonia.org/gorgonia\u0026#34; ) func main() { g := gorgonia.NewGraph() var x, y, z *gorgonia.Node var err error // define the expression  x = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;y\u0026#34;)) if z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } // create a VM to run the program on  machine := gorgonia.NewTapeMachine(g) defer machine.Close() // set initial values then run  gorgonia.Let(x, 2.0) gorgonia.Let(y, 2.5) if err = machine.RunAll(); err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) }$ go run main.go 4.5"},{"uri":"https://gorgonia.org/ja/tutorials/mnist/","title":"簡単なニューラルネットの構築 (MNIST)","tags":[],"description":"","content":" はじめに これは MNIST データセットを使って convocation neural network を段階的に構築し練習する為のチュートリアルです。\n完全なコードは、Gorgonia メインリポジトリの examples ディレクトリにあります。 このチュートリアルの目的はコードを詳細に説明することです。 仕組みの詳細については、次の書籍で見ることができます。 Go Machine Learning Projects\nデータセット このパートではデータセットの読み込みと表示について説明します。ニューラルネットの個所に直接ジャンプしたい場合はスキップして Convolution Neural Net part に進んでください。\n 学習およびテストセットは次からダウンロードできます。 Yann LeCun\u0026rsquo;s MNIST website\n train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)  Every image/label starts with a magic number. The encoding/binary package of the standard library of Go makes it easy to read those files. すべての画像やラベルはマジックナンバーで始まります。Goの標準ライブラリの encoding / binary パッケージを使用するとこれらのファイルを簡単に読み取ることができます。\n\u0026lsquo;mnist\u0026rsquo; パッケージ コモディティとして Gorgonia は examples サブディレクトリにパッケージmnist を作成しました。データから情報を抽出し tensors を作成することをゴールとしています。。\nこの関数 readImageFile は reader に含まれる画像の全体を表すバイト列を作ります。\nよく似た関数 readLabelFile はラベルを展開します。\n// Image holds the pixel intensities of an image. // 255 is foreground (black), 0 is background (white). type RawImage []byte // Label is a digit label in 0 to 9 type Label uint8 func readImageFile(r io.Reader, e error) (imgs []RawImage, err error) func readLabelFile(r io.Reader, e error) (labels []Label, err error) そして2つの関数は RawImage と Label から tensor.Tensor への変換処理を担います。\n prepareX prepareY\nfunc prepareX(M []RawImage, dt tensor.Dtype) (retVal tensor.Tensor) func prepareY(N []Label, dt tensor.Dtype) (retVal tensor.Tensor)  パッケージからエクスポートされる関数は loc からファイル typ を読み取って特定の型のテンソル(float32またはfloat64)を返す Load のみです。\n// Load loads the mnist data into two tensors // // typ can be \u0026#34;train\u0026#34;, \u0026#34;test\u0026#34; // // loc represents where the mnist files are held func Load(typ, loc string, as tensor.Dtype) (inputs, targets tensor.Tensor, err error) パッケージのテスト では簡単なメインファイルを作成してデータがロードされることを検証しましょう。\nテストディレクトリのこのレイアウトは次のとおりです:\n$ ls -alhg * -rw-r--r-- 1 staff 375B Nov 11 13:48 main.go testdata: total 107344 drwxr-xr-x 6 staff 192B Nov 11 13:48 . drwxr-xr-x 4 staff 128B Nov 11 13:48 .. -rw-r--r-- 1 staff 7.5M Jul 21 2000 t10k-images.idx3-ubyte -rw-r--r-- 1 staff 9.8K Jul 21 2000 t10k-labels.idx1-ubyte -rw-r--r-- 1 staff 45M Jul 21 2000 train-images.idx3-ubyte -rw-r--r-- 1 staff 59K Jul 21 2000 train-labels.idx1-ubyte 次にテストデータとトレーニングデータの両方を読み取り、結果のテンソルを表示する単純な Go ファイルを作成します:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;gorgonia.org/gorgonia/examples/mnist\u0026#34; \u0026#34;gorgonia.org/tensor\u0026#34; ) func main() { for _, typ := range []string{\u0026#34;test\u0026#34;, \u0026#34;train\u0026#34;} { inputs, targets, err := mnist.Load(typ, \u0026#34;./testdata\u0026#34;, tensor.Float64) if err != nil { log.Fatal(err) } fmt.Println(typ+\u0026#34; inputs:\u0026#34;, inputs.Shape()) fmt.Println(typ+\u0026#34; data:\u0026#34;, targets.Shape()) } } 実行します:\n$ go run main.go test inputs: (10000, 784) test data: (10000, 10) train inputs: (60000, 784) train data: (60000, 10) テストセットには $28\\times28=784 ピクセルの 60000 枚の写真、\u0026rdquo;one-hot\u0026rdquo; エンコードされた 60000 個のラベル、および 10000 個のテストファイルがあります。\n画像の表現 最初の要素の画像を表示してみましょう:\nimport ( //...  \u0026#34;image\u0026#34; \u0026#34;image/png\u0026#34; \u0026#34;gorgonia.org/gorgonia/examples/mnist\u0026#34; \u0026#34;gorgonia.org/tensor\u0026#34; \u0026#34;gorgonia.org/tensor/native\u0026#34; ) func main() { inputs, targets, err := mnist.Load(\u0026#34;train\u0026#34;, \u0026#34;./testdata\u0026#34;, tensor.Float64) if err != nil { log.Fatal(err) } cols := inputs.Shape()[1] imageBackend := make([]uint8, cols) for i := 0; i \u0026lt; cols; i++ { v, _ := inputs.At(0, i) imageBackend[i] = uint8((v.(float64) - 0.1) * 0.9 * 255) } img := \u0026amp;image.Gray{ Pix: imageBackend, Stride: 28, Rect: image.Rect(0, 0, 28, 28), } w, _ := os.Create(\u0026#34;output.png\u0026#34;) vals, _ := native.MatrixF64(targets.(*tensor.Dense)) fmt.Println(vals[0]) err = png.Encode(w, img) } 基礎となる []float64 バックエンドに簡単にアクセスするために native パッケージを使用しています。この操作では新しいデータは生成されません。\n これにより png ファイルが生成されます: そして 5 であることを示す対応するラベルベクトルは:\n$ go run main.go [0.1 0.1 0.1 0.1 0.1 0.9 0.1 0.1 0.1 0.1] 畳み込みニューラルネット 5層の畳み込みネットワークを構築しています。 $x_0$ は1つ前で定義した入力画像です。\n最初の3つのレイヤー $i$ は次のように定義されます:\n$ x_{i+1} = Dropout(Maxpool(ReLU(Convolution(x_i,W_i)))) $\n$i$ は 0-2 の範囲を取ります。\n4番目のレイヤーは基本的にいくつかのアクティベーションを適度にゼロにするドロップアウトレイヤーです:\n$ x_{4} = Dropout(ReLU(x_3\\cdot W_3)) $\n最後のレイヤーは出力ベクトルを取得するために単純な乗算とソフトマックスを適用します(このベクトルは予測したラベルを表します):\n$ y = softmax(x_4\\cdot W_4)$\nネットワークの変数 学習可能なパラメーターは $W_0,W_1,W_2,W_3,W_4$ です。ネットワークの他の変数はドロップアウト確率 $d_0,d_1,d_2,d_3$ です。\nモデルの変数と出力ノードを保持する構造を作成してみましょう:\ntype convnet struct { g *gorgonia.ExprGraph w0, w1, w2, w3, w4 *gorgonia.Node // weights. the number at the back indicates which layer it\u0026#39;s used for \td0, d1, d2, d3 float64 // dropout probabilities  out *gorgonia.Node } learnables の定義 畳み込みは標準の $3\\times3$ カーネルと32個のフィルターを使用しています。 データセットの画像は白黒なので1つのチャネルのみを使用しています。これは以下の重みの定義につながります。\n $W_0 \\in \\mathbb{R}^{32\\times 1\\times3\\times3}$ は最初の畳み込み演算に $W_1 \\in \\mathbb{R}^{64\\times 32\\times3\\times3}$ は2つ目の畳み込み演算に $W_2 \\in \\mathbb{R}^{128\\times 64\\times3\\times3}$ は3つ目の畳み込み演算に $W_3 \\in \\mathbb{R}^{128*3*3\\times 625}$ 最終的な行列乗算の為に用意していますを準。4D入力を行列 (128x3x3) に変形する必要があります。625は任意の数字です。 $W_4 \\in \\mathbb{R}^{625\\times 10}$ で出力サイズを10個のエントリの単一ベクトルに減らします。  NN 最適化では、出力と入力よりも小さい中間層がある場合に無駄な情報を「圧縮する」ことがよく知られています。 入力は784なので次のレイヤーは小さくする必要があります。625 は格好良い数です。\n ドロップアウトの確率は慣用的な値に固定されています：\n $d_0=0.2$ $d_1=0.2$ $d_2=0.2$ $d_3=0.55$  これで学習可能なプレースホルダーを持った構造を作成できます:\n// Note: gorgonia is abbreviated G in this example for clarity func newConvNet(g *G.ExprGraph) *convnet { w0 := G.NewTensor(g, dt, 4, G.WithShape(32, 1, 3, 3), G.WithName(\u0026#34;w0\u0026#34;), G.WithInit(G.GlorotN(1.0))) w1 := G.NewTensor(g, dt, 4, G.WithShape(64, 32, 3, 3), G.WithName(\u0026#34;w1\u0026#34;), G.WithInit(G.GlorotN(1.0))) w2 := G.NewTensor(g, dt, 4, G.WithShape(128, 64, 3, 3), G.WithName(\u0026#34;w2\u0026#34;), G.WithInit(G.GlorotN(1.0))) w3 := G.NewMatrix(g, dt, G.WithShape(128*3*3, 625), G.WithName(\u0026#34;w3\u0026#34;), G.WithInit(G.GlorotN(1.0))) w4 := G.NewMatrix(g, dt, G.WithShape(625, 10), G.WithName(\u0026#34;w4\u0026#34;), G.WithInit(G.GlorotN(1.0))) return \u0026amp;convnet{ g: g, w0: w0, w1: w1, w2: w2, w3: w3, w4: w4, d0: 0.2, d1: 0.2, d2: 0.2, d3: 0.55, } } The learnables are initialized with some values normally sampled using Glorot et al.\u0026rsquo;s algorithm. For more info: All you need is a good init on Arxiv. Learnablesは、通常Glorotらのアルゴリズムを使用してサンプリングされたいくつかの値で初期化されます。 詳細情報：[必要なのは適切なinitのみ]（https://arxiv.org/pdf/1511.06422.pdf）Arxivで。\n ネットワークの定義 convnet の構造にメソッドを追加することにより、ネットワークを定義できるようになりました:\nNote: わかりやすくするためにエラーチェックは再度削除しています\n// This function is particularly verbose for educational reasons. In reality, you\u0026#39;d wrap up the layers within a layer struct type and perform per-layer activations func (m *convnet) fwd(x *gorgonia.Node) (err error) { var c0, c1, c2, fc *gorgonia.Node var a0, a1, a2, a3 *gorgonia.Node var p0, p1, p2 *gorgonia.Node var l0, l1, l2, l3 *gorgonia.Node // LAYER 0 \t// here we convolve with stride = (1, 1) and padding = (1, 1), \t// which is your bog standard convolution for convnet \tc0, _ = gorgonia.Conv2d(x, m.w0, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}) a0, _ = gorgonia.Rectify(c0) p0, _ = gorgonia.MaxPool2D(a0, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}) l0, _ = gorgonia.Dropout(p0, m.d0) // Layer 1 \tc1, _ = gorgonia.Conv2d(l0, m.w1, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}) a1, _ = gorgonia.Rectify(c1) p1, _ = gorgonia.MaxPool2D(a1, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}) l1, _ = gorgonia.Dropout(p1, m.d1) // Layer 2 \tc2, _ = gorgonia.Conv2d(l1, m.w2, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}) a2, _ = gorgonia.Rectify(c2) p2, _ = gorgonia.MaxPool2D(a2, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}) var r2 *gorgonia.Node b, c, h, w := p2.Shape()[0], p2.Shape()[1], p2.Shape()[2], p2.Shape()[3] r2, _ = gorgonia.Reshape(p2, tensor.Shape{b, c * h * w}) l2, _ = gorgonia.Dropout(r2, m.d2) // Layer 3 \tfc, _ = gorgonia.Mul(l2, m.w3) a3, _ = gorgonia.Rectify(fc) l3, _ = gorgonia.Dropout(a3, m.d3) // output decode \tvar out *gorgonia.Node out, _ = gorgonia.Mul(l3, m.w4) m.out, _ = gorgonia.SoftMax(out) return } ネットワークのトレーニング トレーニングセットから取得した入力は行列 $numExample \\times 784$ です。畳み込み演算子は4Dテンソル BCHW を期待しています。最初にすべき事は入力の形状を変更することです:\nnumExamples := inputs.Shape()[0] inputs.Reshape(numExamples, 1, 28, 28) ネットワークをバッチで学習します。バッチサイズは変数（bs）です。現在のバッチの値とラベルを保持する2つの新しいテンソルを作成します。 次にニューラルネットをインスタンス化します:\ng := gorgonia.NewGraph() x := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(bs, 1, 28, 28), gorgonia.WithName(\u0026#34;x\u0026#34;)) y := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(bs, 10), gorgonia.WithName(\u0026#34;y\u0026#34;)) m := newConvNet(g) m.fwd(x) コスト関数 単純なクロスエントロピーに基づいて期待される出力を要素ごとに乗算し平均化することにより、値を最小化するコスト関数を定義します:\n$cost = -\\dfrac{1}{bs} \\sum_{i=1}^{bs}(pred^{(i)}\\cdot y^{(i)})$\nlosses := gorgonia.Must(gorgonia.HadamardProd(m.out, y)) cost := gorgonia.Must(gorgonia.Mean(losses)) cost = gorgonia.Must(gorgonia.Neg(cost)) 後で使用するためにコストの値にポインターを渡します:\nvar costVal gorgonia.Value gorgonia.Read(cost, \u0026amp;costVal) そして symbolic backpropagation を実行します:\ngorgonia.Grad(cost, m.learnables()...) Learnables はこの様に定義します:\nfunc (m *convnet) learnables() gorgonia.Nodes { return gorgonia.Nodes{m.w0, m.w1, m.w2, m.w3, m.w4} } トレーニングループ まずグラフを実行するための vm と、各ステップで学習可能な値を適応させるためのソルバーが必要です。またソルバーが機能するように2つの学習可能な値に対して実際の値をバインドし、勾配の値を保存する必要があります。\nvm := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...)) solver := gorgonia.NewRMSPropSolver(gorgonia.WithBatchSize(float64(bs))) defer vm.Close() バッチサイズを考慮したエポックを構成するいくつかのバッチを定義します:\nbatches := numExamples / bs 次にトレーニングループを作成します:\nfor i := 0; i \u0026lt; *epochs; i++ { for b := 0; b \u0026lt; batches; b++ { // ...  } } ループの中身: 次に各バッチの入力テンソル ($60000 \\times 784$) から値を抽出する必要があります。 各入力は ($bs\\times 784$) です。最初のバッチは 0 から bs-1 までの値、2番目の bs から 2*bs-1 までの値を保持します。そしてテンソルを4Dテンソルに変形します:\nvar xVal, yVal tensor.Tensor xVal, _ = inputs.Slice(sli{start, end}) yVal, _ = targets.Slice(sli{start, end}) xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28) そしてグラフに値を割り当てます:\ngorgonia.Let(x, xVal) gorgonia.Let(y, yVal) VMとソルバーを実行して重みを調整します。\nvm.RunAll() solver.Step(gorgonia.NodesToValueGrads(m.learnables())) vm.Reset() これで学習できるニューラルネットワークができました。\nおわりに 大量のデータが含まれるためコードの実行は比較的遅くなりますが学習はできています。 Gorgoniaのサンプルディレクトリ で完全なコードを見ることができます。\n重みを保存するには irisチュートリアル で説明されているように load と save の2つのメソッドを作成できます。 そして読者への練習としては、このニューラルネットワークを使う為の小さなユーティリティをコーディングがあります。\nHave fun!\n"},{"uri":"https://gorgonia.org/ja/getting-started/","title":"事始め","tags":[],"description":"Quick start with Gorgonia","content":" Gorgoniaの入手 Gorgoniaはgo-get可能でありgo modulesをサポートしています。 ライブラリとその依存物を取得するには単純に以下を実行します。\n$ go get gorgonia.org/gorgonia 簡単な計算をする為の初めてのコード 配管が正常かどうかを確認する簡単なプログラムを作成します:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;gorgonia.org/gorgonia\u0026#34; ) func main() { g := gorgonia.NewGraph() var x, y, z *gorgonia.Node var err error // define the expression  x = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;y\u0026#34;)) if z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } // create a VM to run the program on  machine := gorgonia.NewTapeMachine(g) defer machine.Close() // set initial values then run  gorgonia.Let(x, 2.0) gorgonia.Let(y, 2.5) if err = machine.RunAll(); err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) } プログラムを実行するとこの結果が出力されるはずです： 4.5\n詳細についてはHello Worldチュートリアルを参照してください。\n"},{"uri":"https://gorgonia.org/ja/about/","title":"Gorgoniaの仕組み","tags":[],"description":"このセクションにはGorgoniaの仕組みを説明することを目的とした記事が含まれています。","content":"このセクションにはGorgoniaの仕組みを説明することを目的とした記事が含まれています。\nこのセクションの記事は次の通り:\n 理解志向である 背景とコンテキストを提供  類似品: 料理の社会史に関する記事\n 計算グラフ  Graphs and *Nodes\n 微分  Gorgonia での勾配計算の仕組み\n "},{"uri":"https://gorgonia.org/ja/about/differentiation/","title":"微分","tags":[],"description":"Gorgonia での勾配計算の仕組み","content":" about このセクションは作業中です。微分の仕組みについて説明します。\n 数式微分  このページは数式微分の仕組みについて説明します\n 自動微分  このページは自動微分の仕組みについて説明します\n "},{"uri":"https://gorgonia.org/ja/tutorials/","title":"チュートリアル","tags":[],"description":"チュートリアル","content":" チュートリアル Gorgoniaのさまざまな使われ方を試し始める為のさまざまなチュートリアル。\nこのチュートリアルは:\n 学習指向 新たな人でも始められる 1つずつのレッスン  類似品: 子供に料理の仕方を教える方法\nチュートリアル  こんにちわ世界   簡単なニューラルネットの構築 (MNIST)   Iris データセットでの多変量線形回帰   "},{"uri":"https://gorgonia.org/ja/how-to/","title":"ハウツー","tags":[],"description":"Various howto solve a specific problem with Gorgonia","content":"Gorgoniaでさまざまな機械学習を行う方法。\nこのセクションではGorgoniaを使用してさまざまな問題を解決する方法を説明します。\n以下の様なハウツーガイドです:\n 目標指向 特定の問題を解決する方法を示します 理解可能な手順で構成します  類似品：料理本のレシピ\n Graphviz (dot) を用いた ExprGraph の描画   データフレームからテンソルを作成 (gota)   Weightsの保存   "},{"uri":"https://gorgonia.org/ja/reference/","title":"リファレンスガイド","tags":[],"description":"これは、Gorgonia のリファレンスガイドです。機材について説明します。","content":" リファレンス これは Gorgonia のリファレンスガイドです。このセクションの記事の目的は次のとおりです:\n 情報指向 機材の説明 正確かつ完全であること  類似品：リファレンス百科事典の記事\n グラフと Exprgraph   発表   Tensor   Solvers   VM   "},{"uri":"https://gorgonia.org/ja/reference/present/","title":"発表","tags":[],"description":"","content":" このページにはプレゼンテーション内で使用できる資料が含まれています。\nロゴ   Logos   logo_g.svg  (22 ko)   logo_g_square.png  (142 ko)   logo_horizontal.svg  (25 ko)   logo_vertical.svg  (25 ko)    "},{"uri":"https://gorgonia.org/ja/how-to/dot/","title":"Graphviz (dot) を用いた ExprGraph の描画","tags":[],"description":"","content":"Gorgonia の encoding パッケージには、ExprGraph を dot language にマーシャリングする関数が含まれています。\nこれにより graphviz プログラムを用いてグラフの png または svg バージョンを生成することができます。\n簡単な方法:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;gorgonia.org/gorgonia\u0026#34; \u0026#34;gorgonia.org/gorgonia/encoding/dot\u0026#34; ) func main() { g := gorgonia.NewGraph() var x, y *gorgonia.Node // define the expression  x = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewScalar(g, gorgonia.Float64, gorgonia.WithName(\u0026#34;y\u0026#34;)) gorgonia.Add(x, y) b, err := dot.Marshal(g) if err != nil { log.Fatal(err) } fmt.Println(string(b)) } このプログラムを実行して出力を dot プロセスに送り込むと画像が生成されます。\n例:\n$ go run main.go | dot -Tsvg \u0026gt; dot-example.svg この画像が出力されます:\n"},{"uri":"https://gorgonia.org/ja/misc/","title":"その他","tags":[],"description":"","content":" Gorgonia に関連するビデオ  Gorgonia, A library that helps facilitate machine learning in Go - Sydney Go Meetup, September 2016 \u0026ldquo;A Funny Thing Happened On The Way To Reimplementing AlphaGo\u0026rdquo; (in Go) by Xuanyi Chew  Gorgonia に言及している記事  Gorgonia (original post on Xuanyi Chew\u0026rsquo;s blog) Tensor Refactor: A Go Experience Report Think like a vertex: using Go\u0026rsquo;s concurrency for graph computation  "},{"uri":"https://gorgonia.org/ja/tutorials/iris/","title":"Iris データセットでの多変量線形回帰","tags":[],"description":"","content":" はじめに Gorgoniaを使用して線形回帰モデルを作成します。\nゴールは以下に与えられた特性を考慮して花の種別を予測することです:\n sepal_length sepal_width petal_length petal_width  存在する種別は以下の通り:\n setosa virginica versicolor  このチュートリアルのゴールはgorgoniaを使用して、与えられたirisデータセットから $\\Theta$ の正しい値を見つけ以下のようなcliユーティリティを作成することです:\n./iris sepal length: 5 sepal width: 3.5 petal length: 1.4 sepal length: 0.2 It is probably a setosa このチュートリアルは学術目的の為の物です。Gorgoniaでこれをどの様にして行うかを説明することがゴールです; これは特定の問題に対する最先端の答えではありません。\n 数学的表現 良くある花弁の長さと幅だけでなく、がく片の長さと幅の関数であった場合とその種別について考察します。\nしたがって $y$ が種別の値であると考える場合に解決すべき方程式は次の通りです:\n$$ y = \\theta_0 + \\theta_1 * sepal\\_length + \\theta_2 * sepal\\_width + \\theta_3 * petal\\_length + \\theta_4 * petal\\_width$$\nベクトルを考慮した場合の $x$ と $\\Theta$ はこうなります:\n$$ x = \\begin{bmatrix} sepal\\_length \u0026amp; sepal\\_width \u0026amp; petal\\_length \u0026amp; petal\\_width \u0026amp; 1\\end{bmatrix}$$\n$$ \\Theta = \\begin{bmatrix} \\theta_4 \\theta_3 \\theta_2 \\theta_1 \\theta_0 \\end{bmatrix} $$\nよってこうなります。\n$$y = x\\cdot\\Theta$$\n線形回帰 正しい値を見つける為に線形回帰を使用します。 データを5列(がく片の長さ、がく片の幅、花弁の長さ、花弁の幅、およびバイアスの1)を含む行列 $X$ にエンコードします。 行列の行は種別を表します。\n対応する種別をfloat値を持つ列ベクトル $Y$ にエンコードします。\n setosa = 1.0 virginica = 2.0 versicolor = 3.0  学習段階ではコストは次のように表す事ができます:\n$cost = \\dfrac{1}{m} \\sum_{i=1}^m(X^{(i)}\\cdot\\Theta-Y^{(i)})^2$\n勾配降下法を使用してコストを下げ $\\Theta$ の正確な値を取得します\n正規方程式での値として $\\theta$ は取得することができます。 $$ \\theta = \\left( X^TX \\right)^{-1}X^Ty $$ gonumでの基本的な実装についてはこのgistを参照してください。\n gota(データフレーム)を使用してトレーニングセットを生成する まずトレーニングデータを生成しましょう。データフレームを使用してスムーズに行います。\nデータフレームの使用方法やその他の情報についてはhowtoを参照\n func getXYMat() (*mat.Dense, *mat.Dense) { f, err := os.Open(\u0026#34;iris.csv\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() df := dataframe.ReadCSV(f) xDF := df.Drop(\u0026#34;species\u0026#34;) toValue := func(s series.Series) series.Series { records := s.Records() floats := make([]float64, len(records)) for i, r := range records { switch r { case \u0026#34;setosa\u0026#34;: floats[i] = 1 case \u0026#34;virginica\u0026#34;: floats[i] = 2 case \u0026#34;versicolor\u0026#34;: floats[i] = 3 default: log.Fatalf(\u0026#34;unknown iris: %v\\n\u0026#34;, r) } } return series.Floats(floats) } yDF := df.Select(\u0026#34;species\u0026#34;).Capply(toValue) numRows, _ := xDF.Dims() xDF = xDF.Mutate(series.New(one(numRows), series.Float, \u0026#34;bias\u0026#34;)) fmt.Println(xDF.Describe()) fmt.Println(yDF.Describe()) return mat.DenseCopyOf(\u0026amp;matrix{xDF}), mat.DenseCopyOf(\u0026amp;matrix{yDF}) } Gorgoniaで使用できる2つの行列を返します。\n式グラフを作成する 方程式 $X\\cdot\\Theta$ は ExprGraph として表されます:\nfunc getXY() (*tensor.Dense, *tensor.Dense) { x, y := getXYMat() xT := tensor.FromMat64(x) yT := tensor.FromMat64(y) // Get rid of the last dimension to create a vector \ts := yT.Shape() yT.Reshape(s[0]) return xT, yT } func main() { xT, yT := getXY() g := gorgonia.NewGraph() x := gorgonia.NodeFromAny(g, xT, gorgonia.WithName(\u0026#34;x\u0026#34;)) y := gorgonia.NodeFromAny(g, yT, gorgonia.WithName(\u0026#34;y\u0026#34;)) theta := gorgonia.NewVector( g, gorgonia.Float64, gorgonia.WithName(\u0026#34;theta\u0026#34;), gorgonia.WithShape(xT.Shape()[1]), gorgonia.WithInit(gorgonia.Uniform(0, 1))) pred := must(gorgonia.Mul(x, theta)) // Saving the value for later use  var predicted gorgonia.Value gorgonia.Read(pred, \u0026amp;predicted) Gorgoniaは高度に最適化されています。良いパフォーマンスを得る為にポインターとメモリを頻繁に使用しています。 したがって実行時(実行プロセス中)に*NodeのValue()メソッドを呼び出すと誤った結果になる可能性があります。 もし実行時(例えば学習段階で)にValueに格納されている*Nodeの特定の値にアクセスする必要がある場合はその参照を保持する必要があります。 これがReadメソッドを使用している理由です。 predictedは $X\\cdot\\Theta$ の結果の値を常に格納しています。\n 勾配計算の準備 Gorgoniaの象徴的な微分機能を使います。\nまずコスト関数を作成し solver を使用して勾配降下を実行しコストを下げます。\nコストを保持するノードの作成 コスト($cost = \\dfrac{1}{m} \\sum_{i=1}^m(X^{(i)}\\cdot\\Theta-Y^{(i)})^2$) を追加することにより exprgraphを補完します。\nsquaredError := must(gorgonia.Square(must(gorgonia.Sub(pred, y)))) cost := must(gorgonia.Mean(squaredError)) このコストを下げたいので $\\Theta$ に関する勾配を評価します:\nif _, err := gorgonia.Grad(cost, theta); err != nil { log.Fatalf(\u0026#34;Failed to backpropagate: %v\u0026#34;, err) } 勾配降下法 勾配降下のメカニズムを使用します。これは勾配を使用してパラメーター $\\Theta$ を段階的に調節することを意味します。\n基本的な勾配降下はGorgoniaのVanilla Solverによって実装されています。 学習率 $\\gamma$ を0.001に設定します。\nsolver := gorgonia.NewVanillaSolver(gorgonia.WithLearnRate(0.001)) そして各ステップで勾配に感謝しつつsolverに $ \\ Theta$ パラメーターを更新するように依頼します。 したがってイテレーションごとにsolverに渡す変数 update を設定します。\n勾配降下はこの方程式に従って各ステップで []gorgonia.ValueGrad に渡されるすべての値を更新します。 ${\\displaystyle x^{(k+1)}=x^{(k)}-\\gamma \\nabla f\\left(x^{(k)}\\right)}$ solverはNodesではなくValuesで動作することを理解する事が重要です。 ただし物事を簡単にする為にValueGradは*Node構造によって実現される interface{} になっています。\n この場合 $\\Theta$ を最適化し次のようにsolverに値を更新する様に依頼します。\n${\\displaystyle \\Theta^{(k+1)}=\\Theta^{(k)}-\\gamma \\nabla f\\left(\\Theta^{(k)}\\right)}$\nそのためには $\\Theta$ をSolverのStepメソッドに渡す必要があります。\nupdate := []gorgonia.ValueGrad{theta} // ... if err = solver.Step(update); err != nil { log.Fatal(err) } The learning iterations 原理が分かりましたね。幾らかの勾配降下の魔法を起こし得る vm を使用して計算を実行する必要があります。\nvm を作成してグラフを実行します(そして勾配計算を行います):\nmachine := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(theta)) defer machine.Close() solverにパラメーター $\\Theta$ についての勾配を更新するように依頼します。 そのためTapeMachineに $\\Theta$ の値(言わば2次元の値)を保存するよう指示しなければなりません。 これは BindDualValues 関数を使用して行います。\n では各ステップでループを作成してグラフを実行しましょう; さぁ機械が学習します!\niter := 1000000 var err error for i := 0; i \u0026lt; iter; i++ { if err = machine.RunAll(); err != nil { fmt.Printf(\u0026#34;Error during iteration: %v: %v\\n\u0026#34;, i, err) break } if err = solver.Step(model); err != nil { log.Fatal(err) } machine.Reset() // Reset is necessary in a loop like this } 幾らかの情報を取得 この呼び出しを使用して学習プロセスの情報をダンプできます\nfmt.Printf(\u0026#34;theta: %2.2f Iter: %v Cost: %2.3f Accuracy: %2.2f \\r\u0026#34;, theta.Value(), i, cost.Value(), accuracy(predicted.Data().([]float64), y.Value().Data().([]float64))) accuracy は以下の様に定義しました:\nfunc accuracy(prediction, y []float64) float64 { var ok float64 for i := 0; i \u0026lt; len(prediction); i++ { if math.Round(prediction[i]-y[i]) == 0 { ok += 1.0 } } return ok / float64(len(y)) } これにより学習プロセス中には以下の様な行が表示されます:\ntheta: [ 0.26 -0.41 0.44 -0.62 0.83] Iter: 26075 Cost: 0.339 Accuracy: 0.61 weightsの保存 訓練が完了したら予測を行えるように $\\Theta$ の値を保存します:\nfunc save(value gorgonia.Value) error { f, err := os.Create(\u0026#34;theta.bin\u0026#34;) if err != nil { return err } defer f.Close() enc := gob.NewEncoder(f) err = enc.Encode(value) if err != nil { return err } return nil } 推論を行う簡単なcliを作る まずは訓練フェーズからパラメータを読み込んでみましょう：\nfunc main() { f, err := os.Open(\u0026#34;theta.bin\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() dec := gob.NewDecoder(f) var thetaT *tensor.Dense err = dec.Decode(\u0026amp;thetaT) if err != nil { log.Fatal(err) } では前に行った様にモデル(exprgraph)を作成します:\n実際のアプリケーションはおそらく別のパッケージでモデルを共有する事になるでしょう\n g := gorgonia.NewGraph() theta := gorgonia.NodeFromAny(g, thetaT, gorgonia.WithName(\u0026#34;theta\u0026#34;)) values := make([]float64, 5) xT := tensor.New(tensor.WithBacking(values)) x := gorgonia.NodeFromAny(g, xT, gorgonia.WithName(\u0026#34;x\u0026#34;)) y, err := gorgonia.Mul(x, theta) そして標準入力から情報を取得するforループに入り計算を実行して結果を表示します:\nmachine := gorgonia.NewTapeMachine(g) values[4] = 1.0 for { values[0] = getInput(\u0026#34;sepal length\u0026#34;) values[1] = getInput(\u0026#34;sepal widt\u0026#34;) values[2] = getInput(\u0026#34;petal length\u0026#34;) values[3] = getInput(\u0026#34;petal width\u0026#34;) if err = machine.RunAll(); err != nil { log.Fatal(err) } switch math.Round(y.Value().Data().(float64)) { case 1: fmt.Println(\u0026#34;It is probably a setosa\u0026#34;) case 2: fmt.Println(\u0026#34;It is probably a virginica\u0026#34;) case 3: fmt.Println(\u0026#34;It is probably a versicolor\u0026#34;) default: fmt.Println(\u0026#34;unknown iris\u0026#34;) } machine.Reset() } 以下は入力を得る為の便利関数です:\nfunc getInput(s string) float64 { reader := bufio.NewReader(os.Stdin) fmt.Printf(\u0026#34;%v: \u0026#34;, s) text, _ := reader.ReadString(\u0026#39;\\n\u0026#39;) text = strings.Replace(text, \u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;, -1) input, err := strconv.ParseFloat(text, 64) if err != nil { log.Fatal(err) } return input } go build や go run を実行できます。そして そう ! 特徴を考慮して、あやめの種別を予測できる完全に自律的なcliになりました:\n$ go run main.go sepal length: 4.4 sepal widt: 2.9 petal length: 1.4 petal width: 0.2 It is probably a setosa sepal length: 5.9 sepal widt: 3.0 petal length: 5.1 petal width: 1.8 It is probably a virginica Conclusion これは段階的な例です。 thetaの初期値を操作したりGorgoniaの中で物事がどの様に行われるのかを見る為にsolverを変更して試してみましょう。\n全体のコードはGorgoniaプロジェクトのexampleで見つける事ができます。.\nボーナス: 視覚的表現 gonum plotter ライブラリを使えばデータセットを可視化する事ができます。 これを実現する方法の簡単な例を次に示します:\nimport ( \u0026#34;gonum.org/v1/plot\u0026#34; \u0026#34;gonum.org/v1/plot/plotter\u0026#34; \u0026#34;gonum.org/v1/plot/plotutil\u0026#34; \u0026#34;gonum.org/v1/plot/vg\u0026#34; \u0026#34;gonum.org/v1/plot/vg/draw\u0026#34; ) func plotData(x []float64, a []float64) []byte { p, err := plot.New() if err != nil { log.Fatal(err) } p.Title.Text = \u0026#34;sepal length \u0026amp; width\u0026#34; p.X.Label.Text = \u0026#34;length\u0026#34; p.Y.Label.Text = \u0026#34;width\u0026#34; p.Add(plotter.NewGrid()) l := len(x) / len(a) for k := 1; k \u0026lt;= 3; k++ { data0 := make(plotter.XYs, 0) for i := 0; i \u0026lt; len(a); i++ { if k != int(a[i]) { continue } x1 := x[i*l+0] // sepal_length \ty1 := x[i*l+1] // sepal_width \tdata0 = append(data0, plotter.XY{X: x1, Y: y1}) } data, err := plotter.NewScatter(data0) if err != nil { log.Fatal(err) } data.GlyphStyle.Color = plotutil.Color(k - 1) data.Shape = \u0026amp;draw.PyramidGlyph{} p.Add(data) p.Legend.Add(fmt.Sprint(k), data) } w, err := p.WriterTo(4*vg.Inch, 4*vg.Inch, \u0026#34;png\u0026#34;) if err != nil { panic(err) } var b bytes.Buffer writer := bufio.NewWriter(\u0026amp;b) w.WriteTo(writer) ioutil.WriteFile(\u0026#34;out.png\u0026#34;, b.Bytes(), 0644) return b.Bytes() }"},{"uri":"https://gorgonia.org/ja/how-to/dataframe/","title":"データフレームからテンソルを作成 (gota)","tags":[],"description":"","content":" このハウツーでは、gotaを使用してデータフレームからテンソルを作成する方法を説明します。 The goal is to read a csv file and create a *tensor.Dense with shape (2,2). ゴールは、csvファイルを読み取り、(2,2) のシェイプの* tensor.Denseを作成することです。\ncsvファイルからデータフレームを作成する 以下のコンテンツのcsvファイルを考えます:\nsepal_length,sepal_width,petal_length,petal_width,species 5.1 ,3.5 ,1.4 ,0.2 ,setosa 4.9 ,3.0 ,1.4 ,0.2 ,setosa 4.7 ,3.2 ,1.3 ,0.2 ,setosa 4.6 ,3.1 ,1.5 ,0.2 ,setosa 5.0 ,3.6 ,1.4 ,0.2 ,setosa ... これはIris flower data setからの抜粋です。 データセットのコピーはここから見つける事ができます。\n 種別以外のすべての値を含むテンソルを作成します。\ngotaを使用してデータフレームを作成する gotaのデータフレームパッケージにはio.Readerを引数として取る関数ReadCSVがあります。\nf, err := os.Open(\u0026#34;iris.csv\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() df := dataframe.ReadCSV(f) dfがファイルに存在する全てのデータが含まれるDataFrameです。\ngotaはデータフレームの列を参照する為にCSVの最初の行を使用します。\n 種別(species)カラムを削除しましょう:\nxDF := df.Drop(\u0026#34;species\u0026#34;) データフレームを行列に変換する 物事を簡単にするために、データフレームをgonumで定義されているMatrixに変換します(matrixのgodocを参照)。 matrixはインタフェースです。gotaのデータフレームはMatrixインターフェイスを満たしません。gotaのドキュメントに記載されているように、 Matrixインターフェイスを満たすために、データフレームのラッパーを作成します。\ntype matrix struct { dataframe.DataFrame } func (m matrix) At(i, j int) float64 { return m.Elem(i, j).Float() } func (m matrix) T() mat.Matrix { return mat.Transpose{Matrix: m} } テンソルを作る データフレームをmatrix構造体の中にラッピングすると関数tensor.FromMat64のおかげで*Denseテンソルを作成できるようになります。\nxT := tensor.FromMat64(mat.DenseCopyOf(\u0026amp;matrix{xDF}))"},{"uri":"https://gorgonia.org/ja/how-to/save-weights/","title":"Weightsの保存","tags":[],"description":"","content":" ゴール このハウツーのゴールはノードの値を保存して復元する方法を説明することです。\n実装 現状できる最善の方法は、対応するノードの値を保存して復元することです。\nテンソルはGobEncodeおよびGobDecodeインターフェースを実現しており、これが最良のオプションです。 バックエンドを要素のスライスとして保存することもできますがこれは少し複雑です。\nこれを行うサンプルコードを以下に示します(まったく最適化していません。自由に修正してください):\npackage main import ( \u0026#34;encoding/gob\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;gorgonia.org/gorgonia\u0026#34; \u0026#34;gorgonia.org/tensor\u0026#34; ) var ( backup = \u0026#34;/tmp/example_gorgonia\u0026#34; ) func main() { g := gorgonia.NewGraph() var x, y, z *gorgonia.Node var err error // Create the graph  x = gorgonia.NewTensor(g, gorgonia.Float64, 2, gorgonia.WithShape(2, 2), gorgonia.WithName(\u0026#34;x\u0026#34;)) y = gorgonia.NewTensor(g, gorgonia.Float64, 2, gorgonia.WithShape(2, 2), gorgonia.WithName(\u0026#34;y\u0026#34;)) if z, err = gorgonia.Add(x, y); err != nil { log.Fatal(err) } // Init variables  xT, yT, err := readFromBackup() if err != nil { log.Println(\u0026#34;cannot read backup, doing init\u0026#34;, err) xT = tensor.NewDense(gorgonia.Float64, []int{2, 2}, tensor.WithBacking([]float64{0, 1, 2, 3})) yT = tensor.NewDense(gorgonia.Float64, []int{2, 2}, tensor.WithBacking([]float64{0, 1, 2, 3})) } err = gorgonia.Let(x, xT) if err != nil { log.Fatal(err) } err = gorgonia.Let(y, yT) if err != nil { log.Fatal(err) } // create a VM to run the program on  machine := gorgonia.NewTapeMachine(g) defer machine.Close() if err = machine.RunAll(); err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%v\u0026#34;, z.Value()) err = save([]*gorgonia.Node{x, y}) if err != nil { log.Fatal(err) } } func readFromBackup() (tensor.Tensor, tensor.Tensor, error) { f, err := os.Open(backup) if err != nil { return nil, nil, err } defer f.Close() dec := gob.NewDecoder(f) var xT, yT *tensor.Dense log.Println(\u0026#34;decoding xT\u0026#34;) err = dec.Decode(\u0026amp;xT) if err != nil { return nil, nil, err } log.Println(\u0026#34;decoding yT\u0026#34;) err = dec.Decode(\u0026amp;yT) if err != nil { return nil, nil, err } return xT, yT, nil } func save(nodes []*gorgonia.Node) error { f, err := os.Create(backup) if err != nil { return err } defer f.Close() enc := gob.NewEncoder(f) for _, node := range nodes { err := enc.Encode(node.Value()) if err != nil { return err } } return nil } 以下が得られる:\n$ go run main.go 2019/10/28 08:07:26 cannot read backup, doing init open /tmp/example_gorgonia: no such file or directory ⎡0 2⎤ ⎣4 6⎦ $ go run main.go 2019/10/28 08:07:29 decoding xT 2019/10/28 08:07:29 decoding yT ⎡0 2⎤ ⎣4 6⎦"},{"uri":"https://gorgonia.org/ja/reference/tensor/","title":"Tensor","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/ja/reference/vm/lispmachine/","title":"LispMachine","tags":[],"description":"","content":"LispMachine はグラフを入力として受け取るように設計されており、グラフのノードで直接実行されます。 グラフが変更された場合は、単純に新しい軽量 LispMachine を作成して実行します。 LispMachine はサイズが固定されていない recurrent neural networks の作成などのタスクに適しています。\nトレードオフとしては LispMachine でのグラフの実行が TapeMachine での実行よりも一般に遅いことです。 グラフの同じ静的な \u0026ldquo;画像\u0026rdquo; が与えられます。\n"},{"uri":"https://gorgonia.org/ja/reference/vm/tapemachine/","title":"Tapemachine","tags":[],"description":"","content":" TapeMachine は一般的にに静的な式を実行するのに役立ちます(つまり計算グラフは変更されない)。 静的な性質があるので TapeMachine は一度だけコンパイルされ何度も式を実行するのに向いています(線形回帰や SVM など)。\n技術詳細 TapeMachine はインストラクションのリストに対してグラフを事前コンパイルします。 その後、命令を線形に順次実行します。 主なトレードオフはダイナミズムです。 再コンパイルプロセスが必要になるため、グラフを動的に作成することはできません(コンパイルは比較的高価なため)。 ただし、コード生成段階で多くの最適化が行われるため TapeMachine で実行されるグラフははるかに高速に実行されます。\n"},{"uri":"https://gorgonia.org/ja/about/differentiation/symbolicdiff/","title":"数式微分","tags":[],"description":"","content":"このページは数式微分の仕組みについて説明します\n"},{"uri":"https://gorgonia.org/ja/about/differentiation/autodiff/","title":"自動微分","tags":[],"description":"","content":"このページは自動微分の仕組みについて説明します\n"},{"uri":"https://gorgonia.org/ja/reference/solver/","title":"Solvers","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/ja/","title":"main","tags":[],"description":"","content":" Gorgonia GorgoniaはGoでの機械学習を楽にするためのライブラリです。\n多次元配列を含む数式を簡単に記述して評価します。\nTheanoまたはTensorFlowに似た物の様に感じるかもしれませんが、それはアイデアが非常に似ているためです。\n具体的にはライブラリはTheanoのようにかなり低レベルですが、Tensorflowの様なより高い目標を持っています。\nなぜGorgoniaを使うのか? Gorgoniaを使用する主な理由は開発者を楽にする事です。Goスタックを広範囲に使用しているのであれば使い慣れた環境で実稼働対応の機械学習システムを作成する機能にアクセスできます。\n大儀ではML/AIは2つのステージに分類されます。1つはさまざまなモデルを構築しテストと再テストを行う実験のステージ。またテストを実施し実際に試され、デプロイされた後のモデルがあるステージ。これらはデータサイエンティストとデータエンジニアのように異なる役割を必要とします。\n通常、2つのフェーズには異なるツールが存在します。Python/Lua（Theano、Torchなどを使用)が一般的に使用されます 実験の段階では、C++(dlib、mlpack等より高性能な言語でモデルが書き換えられます)。もちろん今日では格差は縮まりつつあり、人々は頻繁にツールを共有しています。 Tensorflowはこのギャップを埋めるツールの1つです。\nGorgoniaは同じ場所を目指していますがGo環境を提供します。Gorgoniaは現在とても良い性能を発揮します - その速度はTheanoとTensorflowのCPU実装に匹敵します。 GPUの実装はcgoの負担が重いため比較するには多少手間がかかりますが、これらは現在、積極的に改善を行っている領域ですのでご安心ください。\nこのウェブサイトの構成は? このウェブサイトはさまざまなゴールを持つ4つのセクションで構成されています:\n 事始め  Quick start with Gorgonia\n Gorgoniaの仕組み  このセクションにはGorgoniaの仕組みを説明することを目的とした記事が含まれています。\n チュートリアル  チュートリアル\n ハウツー  Various howto solve a specific problem with Gorgonia\n リファレンスガイド  これは、Gorgonia のリファレンスガイドです。機材について説明します。\n その他   Gorgonia に関連するビデオ Gorgonia, A library that helps facilitate machine learning in Go - Sydney Go Meetup, September 2016 \u0026ldquo;A Funny Thing Happened On The Way To Reimplementing AlphaGo\u0026rdquo; (in Go) by Xuanyi Chew Gorgonia に言及している記事 Gorgonia (original post on Xuanyi Chew\u0026rsquo;s blog) Tensor Refactor: A Go Experience Report Think like a vertex: using Go\u0026rsquo;s concurrency for graph computation  "},{"uri":"https://gorgonia.org/ja/reference/vm/","title":"VM","tags":[],"description":"","content":"Gorgonia の VM は exprgraph を理解し、それを使用して計算を実行する機能を実装したオブジェクトです。\n技術的に言えば、3つのメソッドを持つ interface {} です:\ntype VM interface { RunAll() error Reset() // Close closes all the machine resources (CUDA, if any, loggers if any)  Close() error } 現在のバージョンの Gorgonia にはさまざまな VM があります。\n LispMachine   Tapemachine   機能が異なり、入力も異なります。\n"},{"uri":"https://gorgonia.org/ja/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://gorgonia.org/ja/tags/","title":"Tags","tags":[],"description":"","content":""}]