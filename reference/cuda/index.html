<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.58.3"><meta name=description content="Gorgonia: Deep learning for Go - Write and evaluate mathematical equations involving multidimensional arrays (tensors) easily - this is the reference website with tutorials and howtos"><meta name=author content="The Gorgonia Authors"><link rel="shortcut icon" href=/images/favicon.png type=image/png><title>CUDA support :: Gorgonia</title><link href=/css/nucleus.css?1601728477 rel=stylesheet><link href=/css/fontawesome-all.min.css?1601728477 rel=stylesheet><link href=/css/hybrid.css?1601728477 rel=stylesheet><link href=/css/featherlight.min.css?1601728477 rel=stylesheet><link href=/css/perfect-scrollbar.min.css?1601728477 rel=stylesheet><link href=/css/auto-complete.css?1601728477 rel=stylesheet><link href=/css/atom-one-dark-reasonable.css?1601728477 rel=stylesheet><link href=/css/theme.css?1601728477 rel=stylesheet><link href=/css/hugo-theme.css?1601728477 rel=stylesheet><link href=/css/theme-gorgonia.css?1601728477 rel=stylesheet><script src=/js/jquery-3.3.1.min.js?1601728477></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><meta property=og:title content="CUDA support"><meta property=og:description content="Gorgonia comes with CUDA support out of the box. However, usage is specialized. To use CUDA, you must build your application with the build tag cuda, like so:
go build -tags='cuda' . Furthermore, there are some additional requirements:
 CUDA toolkit is required. Installing this installs the nvcc compiler which is required to run your code with CUDA (Be sure to follow the post-installation steps). go install gorgonia.org/gorgonia/cmd/cudagen. This installs the cudagen program."><meta property=og:type content=article><meta property=og:url content=https://gorgonia.org/reference/cuda/><meta property=og:image content=https://gorgonia.org/images/opengraph.png><meta property=article:published_time content=2020-03-24T17:17:17+01:00><meta property=article:modified_time content=2020-03-24T17:17:17+01:00><meta property=og:site_name content=Gorgonia><meta name=twitter:card content=summary_large_image><meta name=twitter:image content=https://gorgonia.org/images/opengraph.png><meta name=twitter:title content="CUDA support"><meta name=twitter:description content="Gorgonia comes with CUDA support out of the box. However, usage is specialized. To use CUDA, you must build your application with the build tag cuda, like so:
go build -tags='cuda' . Furthermore, there are some additional requirements:
 CUDA toolkit is required. Installing this installs the nvcc compiler which is required to run your code with CUDA (Be sure to follow the post-installation steps). go install gorgonia.org/gorgonia/cmd/cudagen. This installs the cudagen program."></head><body data-url=/reference/cuda/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=https://gorgonia.org/><img src=https://gorgonia.org/images/logo/gorgonia.svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label><input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=/js/lunr.min.js?1601728477></script><script type=text/javascript src=/js/auto-complete.js?1601728477></script><script type=text/javascript>var baseurl="https:\/\/gorgonia.org\/";</script><script type=text/javascript src=/js/search.js?1601728477></script></div><div class=highlightable><ul class=topics><li data-nav-id=/getting-started/ title="Getting Started" class=dd-item><a href=/getting-started/>Getting Started
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/getting-started/ubiquitous-language/ title="Ubiquitous Language and glossary" class=dd-item><a href=/getting-started/ubiquitous-language/>Ubiquitous Language and glossary
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/getting-started/contributing-doc/ title="Start contributing to the doc" class=dd-item><a href=/getting-started/contributing-doc/>Start contributing to the doc
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/about/ title="How Gorgonia works" class=dd-item><a href=/about/>How Gorgonia works
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/about/computation-graph/ title="Computation Graph" class=dd-item><a href=/about/computation-graph/>Computation Graph
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/about/differentiation/ title=Differentiation class=dd-item><a href=/about/differentiation/>Differentiation
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/about/differentiation/autodiff/ title="Automatic Differentiation" class=dd-item><a href=/about/differentiation/autodiff/>Automatic Differentiation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/about/differentiation/symbolicdiff/ title="Symbolic Differentiation" class=dd-item><a href=/about/differentiation/symbolicdiff/>Symbolic Differentiation
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li><li data-nav-id=/tutorials/ title=Tutorials class=dd-item><a href=/tutorials/>Tutorials
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/tutorials/hello-world/ title="Hello World" class=dd-item><a href=/tutorials/hello-world/>Hello World
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/tutorials/mnist/ title="Simple Convolution Neural Net (MNIST)" class=dd-item><a href=/tutorials/mnist/>Simple Convolution Neural Net (MNIST)
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/tutorials/mnist-cuda/ title="Convnet with CUDA" class=dd-item><a href=/tutorials/mnist-cuda/>Convnet with CUDA
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/tutorials/iris/ title="Multivariate linear regression on Iris Dataset" class=dd-item><a href=/tutorials/iris/>Multivariate linear regression on Iris Dataset
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/how-to/ title="How To" class=dd-item><a href=/how-to/>How To
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/how-to/troubleshoot-gpu-issues/ title="Troubleshoot GPU Issues" class=dd-item><a href=/how-to/troubleshoot-gpu-issues/>Troubleshoot GPU Issues
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/how-to/dot/ title="Drawing the ExprGraph with Graphviz (dot)" class=dd-item><a href=/how-to/dot/>Drawing the ExprGraph with Graphviz (dot)
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/how-to/dataframe/ title="Create a tensor from a Dataframe (gota)" class=dd-item><a href=/how-to/dataframe/>Create a tensor from a Dataframe (gota)
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/how-to/save-weights/ title="Save Weights" class=dd-item><a href=/how-to/save-weights/>Save Weights
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/how-to/autodiff/ title="How to compute gradient (differentiation)" class=dd-item><a href=/how-to/autodiff/>How to compute gradient (differentiation)
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/reference/ title="Reference guide" class="dd-item
        parent"><a href=/reference/>Reference guide
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/reference/exprgraph/ title="Graph / Exprgraph" class=dd-item><a href=/reference/exprgraph/>Graph / Exprgraph
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/reference/present/ title=Present class=dd-item><a href=/reference/present/>Present
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/reference/cuda/ title="CUDA support" class="dd-item active"><a href=/reference/cuda/>CUDA support
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/reference/tensor/ title=Tensor class=dd-item><a href=/reference/tensor/>Tensor
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/reference/solver/ title=Solvers class=dd-item><a href=/reference/solver/>Solvers
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/reference/vm/ title=VM class=dd-item><a href=/reference/vm/>VM
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/reference/vm/gomachine/ title="Go Machine" class=dd-item><a href=/reference/vm/gomachine/>Go Machine
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/reference/vm/lispmachine/ title=LispMachine class=dd-item><a href=/reference/vm/lispmachine/>LispMachine
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/reference/vm/tapemachine/ title=Tapemachine class=dd-item><a href=/reference/vm/tapemachine/>Tapemachine
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li><li data-nav-id=/misc/ title=Miscellaneous class=dd-item><a href=/misc/>Miscellaneous
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://github.com/gorgonia/gorgonia><i class="fab fa-fw fa-github"></i>GitHub repo</a></li><li><a class=padding href="https://pkg.go.dev/gorgonia.org/gorgonia?tab=doc"><i class="fas fa-fw fa-bookmark"></i>API Doc</a></li><li><a class=padding href=http://gophers.slack.com/messages/gorgonia><i class="fab fa-fw fa-slack"></i>Contact us</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value;">
<option id=en value=https://gorgonia.org/reference/cuda/ selected>English</option></select><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><path d="M0 63.75l127.5 127.5L255 63.75z" /></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i>Clear History</a></li></ul></section><section id=footer><center><a class=github-button href=https://github.com/gorgonia/gorgonia data-icon=octicon-star data-show-count=true aria-label="Star gorgonia/gorgonia on GitHub">Star</a><p>Built with <a href=https://github.com/matcornic/hugo-theme-learn><i class="fas fa-heart"></i></a>from <a href=https://getgrav.org>Grav</a> and <a href=https://gohugo.io/>Hugo</a></p></center><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=top-github-link><a class=github-link title="Edit this page" href=https://github.com/gorgonia/gorgonia.github.io/edit/develop/content/reference/cuda.md target=blank><i class="fas fa-code-branch"></i><span id=top-github-link-text>Edit this page</span></a></div><div id=breadcrumbs itemscope itemtype=http://schema.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i></a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span><span class=links><a href=/>main</a> > <a href=/reference/>Reference guide</a> > CUDA support</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#rationale>Rationale</a></li><li><a href=#about-cudagen>About <code>cudagen</code></a></li><li><a href=#op-s-supported-by-cuda><code>Op</code>s supported by CUDA</a></li><li><a href=#cuda-improvements>CUDA improvements</a></li></ul></li><li><a href=#example>Example</a></li></ul></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>CUDA support</h1><p>Gorgonia comes with CUDA support out of the box. However, usage is specialized.
To use CUDA, you must build your application with the build tag <code>cuda</code>, like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>go build -tags<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;cuda&#39;</span> .</code></pre></div><p>Furthermore, there are some additional requirements:</p><ul><li><a href=https://developer.nvidia.com/cuda-toolkit>CUDA toolkit</a> is required. Installing this installs the <code>nvcc</code> compiler which is required to run your code with CUDA (Be sure to follow the <a href=http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions>post-installation steps</a>).</li><li><code>go install gorgonia.org/gorgonia/cmd/cudagen</code>. This installs the <code>cudagen</code> program.</li><li>Running <code>cudagen</code> will generate the relevant CUDA related code for Gorgonia. Note that you will need a folder at <code>src\gorgonia.org\gorgonia\cuda modules\target</code></li><li>Only certain ops are supported by the CUDA driver by now. They are implemented in a seperate <a href=https://godoc.org/github.com/gorgonia/gorgonia/ops/nn><code>ops/nn</code> package</a>.</li></ul><div class="notices warning"><p>CUDA requires thread affinity, and therefore the OS thread must be locked. <code>runtime.LockOSThread()</code> must be called in the main function where the VM is running. Please cf this <a href=https://github.com/golang/go/wiki/LockOSThread>wiki</a> for a general information on how to handle this properly within your Go program</p></div><h3 id=rationale>Rationale</h3><p>The main reasons for having such complicated requirements for using CUDA is quite simply performance related. As Dave Cheney famously wrote, <a href=https://dave.cheney.net/2016/01/18/cgo-is-not-go>cgo is not Go</a>. To use CUDA, cgo is unfortunately required. And to use cgo, plenty of tradeoffs need to be made.</p><p>Therefore the solution was to nestle the CUDA related code in a build tag, <code>cuda</code>. That way by default no cgo is used (well, kind-of - you could still use <code>cblas</code> or <code>blase</code>).</p><h3 id=about-cudagen>About <code>cudagen</code></h3><p>The reason for requiring <a href=https://developer.nvidia.com/cuda-toolkit>CUDA toolkit</a> and the tool cudagen is because there are many CUDA <a href=http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities>Compute Capabilities</a>, and generating code for them all would yield a huge binary for no real good reason. Rather, users are encouraged to compile for their specific Compute Capabilities.</p><p><div class="notices info"><p>The reason for requiring an explicit specification to use CUDA for which ops is due to the cost of cgo calls. Additional work is being done currently to implement batched cgo calls, but until that is done, the solution is keyhole &ldquo;upgrade&rdquo; of certain ops</p></div>Lastly, the reason for requiring an explicit specification to use CUDA for which ops is due to the cost of cgo calls. Additional work is being done currently to implement batched cgo calls, but until that is done, the solution is keyhole &ldquo;upgrade&rdquo; of certain ops</p><h3 id=op-s-supported-by-cuda><code>Op</code>s supported by CUDA</h3><p>As of now, only the very basic simple ops support CUDA:</p><p>Elementwise unary operations:</p><ul><li><code>abs</code></li><li><code>sin</code></li><li><code>cos</code></li><li><code>exp</code></li><li><code>ln</code></li><li><code>log2</code></li><li><code>neg</code></li><li><code>square</code></li><li><code>sqrt</code></li><li><code>inv</code> (reciprocal of a number)</li><li><code>cube</code></li><li><code>tanh</code></li><li><code>sigmoid</code></li><li><code>log1p</code></li><li><code>expm1</code></li><li><code>softplus</code></li></ul><p>Elementwise binary operations - only arithmetic operations support CUDA:</p><ul><li><code>add</code></li><li><code>sub</code></li><li><code>mul</code></li><li><code>div</code></li><li><code>pow</code></li></ul><p>From a lot of profiling of this author&rsquo;s personal projects, the ones that really matter are <code>tanh</code>, <code>sigmoid</code>, <code>expm1</code>, <code>exp</code> and <code>cube</code> - basically the activation functions. The other operations do work fine with MKL+AVX and aren&rsquo;t the major cause of slowness in a neural network</p><h3 id=cuda-improvements>CUDA improvements</h3><p>In a trivial benchmark, careful use of CUDA (in this case, used to call <code>sigmoid</code>) shows impressive improvements over non-CUDA code (bearing in mind the CUDA kernel is extremely naive and not optimized):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-test data-lang=test>BenchmarkOneMilCUDA-8   	     300	   3348711 ns/op
BenchmarkOneMil-8       	      50	  33169036 ns/op</code></pre></div><h2 id=example>Example</h2><p>see this <a href=/tutorials/mnist-cuda/>tutorial</a> for a complete example</p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=/reference/vm/tapemachine/ title=Tapemachine><i class="fa fa-chevron-left"></i></a><a class="nav nav-next" href=/reference/tensor/ title=Tensor style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=/js/clipboard.min.js?1601728477></script><script src=/js/perfect-scrollbar.min.js?1601728477></script><script src=/js/perfect-scrollbar.jquery.min.js?1601728477></script><script src=/js/jquery.sticky.js?1601728477></script><script src=/js/featherlight.min.js?1601728477></script><script src=/js/modernizr.custom-3.6.0.js?1601728477></script><script src=/js/learn.js?1601728477></script><script src=/js/hugo-learn.js?1601728477></script><link href=/mermaid/mermaid.css?1601728477 rel=stylesheet><script src=/mermaid/mermaid.js?1601728477></script><script>mermaid.initialize({startOnLoad:true});</script></body></html>